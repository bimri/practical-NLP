{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will demostrate how to perform tokenization,stemming,lemmatization and pos_tagging using libraries like [spacy](https://spacy.io/) and [nltk](https://www.nltk.org/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk==3.2.5\n",
      "  Downloading nltk-3.2.5.tar.gz (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m379.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in /home/bimri/anaconda3/lib/python3.9/site-packages (from nltk==3.2.5) (1.16.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.2.5-py3-none-any.whl size=1392141 sha256=a1ee85e30769e55835f1bd36c8b70165a769a8bea7d632315d7522d61985a048\n",
      "  Stored in directory: /home/bimri/.cache/pip/wheels/d5/98/25/45a61053059ba185df5c45ed819974b68d16c0c3bc54e7060a\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.7\n",
      "    Uninstalling nltk-3.7:\n",
      "      Successfully uninstalled nltk-3.7\n",
      "Successfully installed nltk-3.2.5\n",
      "Collecting spacy==2.2.4\n",
      "  Using cached spacy-2.2.4.tar.gz (6.1 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting murmurhash<1.1.0,>=0.28.0 (from spacy==2.2.4)\n",
      "  Using cached murmurhash-1.0.9-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy==2.2.4)\n",
      "  Using cached cymem-2.0.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy==2.2.4)\n",
      "  Using cached preshed-3.0.8-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (128 kB)\n",
      "Collecting thinc==7.4.0 (from spacy==2.2.4)\n",
      "  Using cached thinc-7.4.0-cp39-cp39-linux_x86_64.whl\n",
      "Collecting blis<0.5.0,>=0.4.0 (from spacy==2.2.4)\n",
      "  Using cached blis-0.4.1-cp39-cp39-linux_x86_64.whl\n",
      "Collecting wasabi<1.1.0,>=0.4.0 (from spacy==2.2.4)\n",
      "  Using cached wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
      "Collecting srsly<1.1.0,>=1.0.2 (from spacy==2.2.4)\n",
      "  Using cached srsly-1.0.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209 kB)\n",
      "Collecting catalogue<1.1.0,>=0.0.7 (from spacy==2.2.4)\n",
      "  Using cached catalogue-1.0.2-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/bimri/anaconda3/lib/python3.9/site-packages (from spacy==2.2.4) (4.64.1)\n",
      "Requirement already satisfied: setuptools in /home/bimri/anaconda3/lib/python3.9/site-packages (from spacy==2.2.4) (63.4.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/bimri/anaconda3/lib/python3.9/site-packages (from spacy==2.2.4) (1.19.5)\n",
      "Collecting plac<1.2.0,>=0.9.6 (from spacy==2.2.4)\n",
      "  Using cached plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/bimri/anaconda3/lib/python3.9/site-packages (from spacy==2.2.4) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/bimri/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/bimri/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/bimri/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/bimri/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (2022.9.14)\n",
      "Building wheels for collected packages: spacy\n",
      "  Building wheel for spacy (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for spacy: filename=spacy-2.2.4-cp39-cp39-linux_x86_64.whl size=10987623 sha256=05dd3d0bb6536f48fea02d3ff64d8f91302a8032894f1e0dea9434ea998022d7\n",
      "  Stored in directory: /home/bimri/.cache/pip/wheels/ad/07/92/5f1eb398c6e7124a9cf7af79aa2c0d44a307bb229d1b361428\n",
      "Successfully built spacy\n",
      "Installing collected packages: wasabi, plac, cymem, srsly, murmurhash, catalogue, blis, preshed, thinc, spacy\n",
      "Successfully installed blis-0.4.1 catalogue-1.0.2 cymem-2.0.7 murmurhash-1.0.9 plac-1.1.3 preshed-3.0.8 spacy-2.2.4 srsly-1.0.6 thinc-7.4.0 wasabi-0.10.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk==3.2.5\n",
    "!pip install spacy==2.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will be our corpus which we will work on\n",
    "corpus_original = \"Need to finalize the demo corpus which will be used for this notebook and it should be done soon !!. It should be done by the ending of this month. But will it? This notebook has been run 4 times !!\"\n",
    "corpus = \"Need to finalize the demo corpus which will be used for this notebook & should be done soon !!. It should be done by the ending of this month. But will it? This notebook has been run 4 times !!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "need to finalize the demo corpus which will be used for this notebook & should be done soon !!. it should be done by the ending of this month. but will it? this notebook has been run 4 times !!\n"
     ]
    }
   ],
   "source": [
    "#lower case the corpus\n",
    "corpus = corpus.lower()\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "need to finalize the demo corpus which will be used for this notebook & should be done soon !!. it should be done by the ending of this month. but will it? this notebook has been run  times !!\n"
     ]
    }
   ],
   "source": [
    "#removing digits in the corpus\n",
    "import re\n",
    "corpus = re.sub(r'\\d+','', corpus)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "need to finalize the demo corpus which will be used for this notebook  should be done soon  it should be done by the ending of this month but will it this notebook has been run  times \n"
     ]
    }
   ],
   "source": [
    "#removing punctuations\n",
    "import string\n",
    "corpus = corpus.translate(str.maketrans('', '', string.punctuation))\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'need to finalize the demo corpus which will be used for this notebook should be done soon it should be done by the ending of this month but will it this notebook has been run times'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing trailing whitespaces\n",
    "corpus = ' '.join([token for token in corpus.split()])\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 contains an egg fragment with a non-PEP 508 name pip 25.0 will enforce this behaviour change. A possible replacement is to use the req @ url syntax, and remove the egg fragment. Discussion can be found at https://github.com/pypa/pip/issues/11617\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting en_core_web_sm==2.2.5\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m338.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /home/bimri/anaconda3/lib/python3.9/site-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/bimri/anaconda3/lib/python3.9/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/bimri/anaconda3/lib/python3.9/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/bimri/anaconda3/lib/python3.9/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.8)\n",
      "Requirement already satisfied: thinc==7.4.0 in /home/bimri/anaconda3/lib/python3.9/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/bimri/anaconda3/lib/python3.9/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/bimri/anaconda3/lib/python3.9/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.10.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/bimri/anaconda3/lib/python3.9/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.6)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/bimri/anaconda3/lib/python3.9/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/bimri/anaconda3/lib/python3.9/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.64.1)\n",
      "Requirement already satisfied: setuptools in /home/bimri/anaconda3/lib/python3.9/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (63.4.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/bimri/anaconda3/lib/python3.9/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/bimri/anaconda3/lib/python3.9/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/bimri/anaconda3/lib/python3.9/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/bimri/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/bimri/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/bimri/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/bimri/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2022.9.14)\n",
      "Building wheels for collected packages: en_core_web_sm\n",
      "  Building wheel for en_core_web_sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en_core_web_sm: filename=en_core_web_sm-2.2.5-py3-none-any.whl size=12011717 sha256=7266ea0f424636c501f359938a07324f9a95775503190df982bdf94f59ee76e8\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-yc23ebw8/wheels/57/07/ed/22b6eecc1314cf0e1cbfb45ebeb5ba063dfbc86645f6f23c23\n",
      "Successfully built en_core_web_sm\n",
      "Installing collected packages: en_core_web_sm\n",
      "Successfully installed en_core_web_sm-2.2.5\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "##NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "\n",
    "tokenized_corpus_nltk = word_tokenize(corpus)\n",
    "print(\"\\nNLTK\\nTokenized corpus:\",tokenized_corpus_nltk)\n",
    "tokenized_corpus_without_stopwords = [i for i in tokenized_corpus_nltk if not i in stop_words_nltk]\n",
    "print(\"Tokenized corpus without stopwords:\",tokenized_corpus_without_stopwords)\n",
    "\n",
    "\n",
    "##SPACY \n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import spacy\n",
    "spacy_model = spacy.load('en_core_web_sm')\n",
    "\n",
    "stopwords_spacy = spacy_model.Defaults.stop_words\n",
    "print(\"\\nSpacy:\")\n",
    "tokenized_corpus_spacy = word_tokenize(corpus)\n",
    "print(\"Tokenized Corpus:\",tokenized_corpus_spacy)\n",
    "tokens_without_sw= [word for word in tokenized_corpus_spacy if not word in stopwords_spacy]\n",
    "\n",
    "print(\"Tokenized corpus without stopwords\",tokens_without_sw)\n",
    "\n",
    "\n",
    "print(\"Difference between NLTK and spaCy output:\\n\",\n",
    "      set(tokenized_corpus_without_stopwords)-set(tokens_without_sw))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the difference output after stopword removal using nltk and spacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Stemming:\n",
      "need to finalize the demo corpus which will be used for this notebook should be done soon it should be done by the ending of this month but will it this notebook has been run times\n",
      "After Stemming:\n",
      "need to final the demo corpu which will be use for thi notebook should be done soon it should be done by the end of thi month but will it thi notebook ha been run time "
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stemmer= PorterStemmer()\n",
    "\n",
    "print(\"Before Stemming:\")\n",
    "print(corpus)\n",
    "\n",
    "print(\"After Stemming:\")\n",
    "for word in tokenized_corpus_nltk:\n",
    "    print(stemmer.stem(word),end=\" \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "for word in tokenized_corpus_nltk:\n",
    "    print(lemmatizer.lemmatize(word),end=\" \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS tagging using spacy\n",
    "print(\"POS Tagging using spacy:\")\n",
    "doc = spacy_model(corpus_original)\n",
    "# Token and Tag\n",
    "for token in doc:\n",
    "    print(token,\":\", token.pos_)\n",
    "\n",
    "#pos tagging using nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "print(\"POS Tagging using NLTK:\")\n",
    "pprint(nltk.pos_tag(word_tokenize(corpus_original)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various other libraries you can use to perform these common pre-processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
